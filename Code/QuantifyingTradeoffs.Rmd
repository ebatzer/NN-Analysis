---
title: "Vector Analysis"
author: "Evan Batzer"
date: "December 13, 2018"
output:
  rmarkdown::html_document:
    theme: lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(smatr); library(testthat); library(vegan)
library(tidyverse); library(fastDummies); library(grid)
library(data.table); library(dtplyr); library(gridExtra);
```

Compiled on `r date()`

# Fertilization Vector Analysis

## Results of statistical consultation

Per discussions with Neil Willits, the two different ways that I could try to 
analyze the nutrient enrichment tradeoff include:

* Conducting a PCA on each site and trying to evaluate nutrient enrichment
change as a linear model. In this case, we have Y = PC1 score (or some other
principal component axis), and $$Y = \mu + \beta_i t + f_i(t)$$, where $\beta_i$ 
is the effect of nutrient enrichment over time and $f_i$ is a nonlinear term that 
reflects the fact that the relationship between time and response isn't apt to be linear.

    * To evaluate the tradeoff between nutrient addition treatments (or if this surface
    exists at all!), we can compare the values of the beta coefficient. For example,
    $\beta_N$ - $\beta_P$ is equal to the tradeoff on this axis. The more negative
    value this is, the greater of a tradeoff we see.
    
    * However, I'm not crazy about this approach, in large part because of the
    difficulty in picking the appropriate principal components. How can we standardize
    this process over multiple sites? Without running the analysis over all of the
    total variation between sites, this is not clear to me.
    
* Another way to do this analysis is quite similar to what we had talked about in 
our last meeting. But rather than actually calculating the multivariate angle by
the formula:
$$\cos (\theta) = \frac{(\vec{V_N} \cdot \vec{V_P})}{\left\lVert\vec{V_N}
\right\rVert \left\lVert\vec{V_P}\right\rVert}$$ we can instead just focus on the dot
product itself, or the ratio relative to the magnitude of each vector. 

    * Neil suggested that the computation that derives the angle $\theta$ can be
    a little unstable. Focusing on the dot product (if we divide by the magnitude
    or convert to unit length) can give us a value between -1 and 1, which indicates 
    travel in either the same or opposite directions.
    
    * I like this approach more, because it doesn't rely on a dimensionality reduction, or
    a summary statistic like grass/forb/legume cover. 

## PermANOVA analysis

__Steps:__

1. Run permANOVA for each site, specifying:

    * Permutations only within blocks
    * Model formula of Y ~ Year + Average Treatment Effect
        * Can also specify a linear term of treatment, or variation across years
        
2. Summarize this data by tabulating each time the permANOVA produces significant
results. Which sites show community change by N, P, and K? Should we just subset
to these sites with significant results?

3. Extract vectors of community change in response to N, P, and K.

  * Also attach whether this estimated effect was significant
  * Standardize to unit length?

4. Calculate the dot product for each pairwise set of vectors. Store this value
(should likely be roughly bounded between -1 and 1)

5. To establish which sorts of sites seem to experience this tradeoff, we can perform
some sort of hierarchical clustering algorithm. Neil suggested using conditional 
inference trees with some sort of univariate control, as opposed to the standard 
Bonferroni method.

## Fitting PerMANOVA to these sites

To extract estimated species responses to treatment, I'm following a similar protocol
as we did in the NutNet meeting this summer. After fitting the PerMANOVA, I've extracted
all of the estimated species responses to the three nutrients we're interested in - N, P, and K. 

The response coefficient is the estimated mean response of that species over all 
years of sampling (at least 5). I've also standardized total observations per column - 
it may make sense to not just look at absolute cover, but rather, the relative % 
cover of species in each observed plot.

# Reading in Datasets

(Code omitted)

```{r}
# Biomass data
biomass_data <- fread('C:/Users/ebatz/Dropbox/NutNet Data/comb-by-plot-25-January-2019.csv',
                  stringsAsFactors = FALSE,
                  na.strings = c('NA','NULL'))

# Cover data
cover <- fread('C:/Users/ebatz/Dropbox/NutNet Data/full-cover-25-January-2019.csv',
               stringsAsFactors = FALSE,
                  na.strings = c('NA','NULL'))

# Plot descriptions
plot.descriptions <- read.csv("C:/Users/ebatz/Dropbox/NutNet Data/comb-by-plot-25-January-2019.csv",
                              stringsAsFactors = FALSE)

# Soil characteristics
soil.chars <- read.csv("C:/Users/ebatz/Dropbox/NutNet Data/comb-by-plot-clim-soil-diversity-25-Jan-2019.csv",
                              stringsAsFactors = FALSE)

# Site covariates
site.covars <- read.csv("../Data/site_covars.csv",
                        stringsAsFactors = FALSE)
```

# Cleaning datasets

```{r}
# Converting some columns to factor prior to joining
biomass_data$site_code <- factor(biomass_data$site_code)
biomass_data$trt <- factor(biomass_data$trt)

#Choose sites with at least 3 years of data
sites <- biomass_data %>% 
  group_by(site_code,region,first_nutrient_year) %>% 
  summarise(yrs.data = length(unique(year)),fyear = min(year), lyear = max(year)) %>% 
  arrange(yrs.data)

sites.long <- as.character(sites[yrs.data >= 3,`site_code`])

bmass <- biomass_data[site_code %in% sites.long]


cover.long <- cover[site_code%in%c(sites.long) & live==1,]
cover.long <- left_join(cover.long,distinct(bmass[,.(`site_code`,`region`)]))
cover.long$max_cover = as.numeric(cover.long$max_cover)

rm(sites)

```

```{r}
#Choosing only our treatments of interest
cover.long <- cover.long[trt %in% c('Control','N','P','K')]

#Clean out unknowns, non-vascular plants
sps <- unique(cover.long$Taxon)
unwanted.sps <- sps[c(grep('UNKNOWN',sps),grep('BRYOPHYTE',sps),grep('LICHEN',sps))]
cover.long <- cover.long[!(Taxon %in% unwanted.sps | local_lifeform == 'MOSS' | functional_group == 'BRYOPHYTE')]
cover.long[functional_group=='GRAMINOID', functional_group := 'GRASS']
rm(sps,unwanted.sps)
rm(sps,unwanted.sps)

#Choose plots with at least 3 years of data
## This takes care of a few sites that have strange plot configurations,
## like sgs.us
site.plots.keep <- cover.long %>% 
  group_by(site_code,block,plot,trt) %>% 
  summarise(yrs.data = length(unique(year))) %>% 
  filter(yrs.data>= 5)

## Some sites have multiple control plots in a block.
## Choosing only the lowest numbered plot belong to a treatment in each block.
site.plots.keep <- left_join(site.plots.keep,
                             site.plots.keep %>% group_by(site_code,block,trt) %>% 
                               summarize(min.plot = min(plot))) %>% 
  filter(plot==min.plot) %>% 
  select(-min.plot,-yrs.data)

cover.long <- left_join(site.plots.keep,cover.long)

cover.long <- droplevels(cover.long) #cover.long$site_code <- factor(cover.long$site_code)
rm(site.plots.keep)

# cast long into wide
cover.wide <- dcast(cover.long,site_code+year+block+plot+trt+year_trt ~ Taxon,
                    value.var='max_cover', 
                    fun.aggregate = sum,drop=T,fill=0)
```


```{r}
# Adding in additional info

siteinfo <- biomass_data[site_code %in% sites.long, 
                         .(`site_code`,`site_name`,`continent`,`region`,
                           `first_nutrient_year`,`site_richness`)] %>% 
  distinct()

spsinfo <- select(data.frame(cover.long),site_code,Family:ps_path,-live) %>% distinct()

```

```{r}
cols <- c(colnames(cover.wide)[-c(1:6)])
cover.wide[ , (cols) := lapply(.SD, "as.numeric"), .SDcols = cols]
attr.mat = cover.wide[,1:6]
cover.vals = data.frame(cover.wide[,-c(1:6)])

# Should we normalize?
# cover.mat = round(vegan::decostand(cover.vals, method = "total"), 3)
cover.mat = cover.vals

attr.mat = dummy_columns(attr.mat, select_columns = "trt", 
                           remove_first_dummy = TRUE) %>%
  replace_na(list(trt_K = 0, trt_N = 0, trt_P = 0)) %>%
  mutate(trt_K_num = as.numeric(trt_K * year_trt),
         trt_P_num = as.numeric(trt_P * year_trt),
         trt_N_num = as.numeric(trt_N * year_trt),
         year_trt = as.numeric(year_trt))

attr.mat$trt_K[attr.mat$year_trt == 0] = 0
attr.mat$trt_P[attr.mat$year_trt == 0] = 0
attr.mat$trt_N[attr.mat$year_trt == 0] = 0

# write.csv(x = bind_cols(attr.mat, cover.mat),
#           file = "../data/compmat_edited.csv")

# For all unique sites selected
panov <- function(x, cover.mat, attr.mat){

  # Subset to a single site
  com.subset <- data.frame(cover.mat)[attr.mat$site_code == x,]
  
  # Remove all zero columns
  com.subset = com.subset[,colSums(com.subset) > 0]
  
  # Pull out relevant plot attributes for each site
  attr.subset <- data.frame(attr.mat) %>% filter(site_code == x)
  
  # Check that these two matrices are the same size
  expect_true(nrow(attr.subset) == nrow(com.subset))
  
  mod_anov <- adonis(com.subset ~ as.factor(year_trt) + trt_K_num + trt_P_num + trt_N_num,
                     data = attr.subset, 
                     se = FALSE, method = "euclidean", 
                     strata = attr.subset$block,
                     by = "margin",
                     permutations = 9999)
    
  # Saving ouput
  output = list(siteinfo = attr.subset,
                aovtable = mod_anov$aov.tab,
                sitescores = mod_anov$coef.sites,
                specscores = mod_anov$coefficients) 
}

sitenames = as.character(unique(attr.mat$site_code))
output = lapply(X = sitenames, FUN = panov, cover.mat = cover.mat, attr.mat = attr.mat)
beepr::beep(sound = 3)
```

```{r}
storagelist <- list()

for(i in 1:length(output)){

  anovatab = bind_cols(site = as.character(unique(output[[i]]$siteinfo$site_code)),
            data.frame(matrix(output[[i]]$aovtable$`Pr(>F)`, nrow = 1)))
  
  colnames(anovatab)[2:ncol(anovatab)] =   rownames(output[[i]]$aovtable)
  
  storagelist[[i]] = anovatab

  }

write.csv(x = bind_rows(storagelist),
  "composition_pvalues.csv")

storagelist <- list()

for(i in 1:length(output)){
  sp_scores = data.frame(output[[i]]$specscores[grep("trt_", rownames(output[[i]]$specscores)),])

  spectab = bind_cols(site = rep(unique(output[[i]]$siteinfo$site_code), 3),
                    trt = rownames(sp_scores),
                    sp_scores)
  
  storagelist[[i]] = spectab
}

specscores_full = bind_rows(storagelist) %>% 
  replace(., is.na(.), 0) %>%
  mutate(trt = gsub("_num", "", trt))

head(specscores_full)[,1:5]
```

## Calculate dot-product of pairwise correlations

Given these estimated parameters, we can calculate the correlation between species responses
to different pairs of nutrient treatments through the dot-product of the fitted
vectors between treatments within a site.

After calculating this dot-product, the distributions of each set correlation scores can be seen in histograms below:

* Values close to zero indicate that species responses to two treatments within  a site are not correlated with one another.

* Values close to one or negative one indicate that species responses to two treatments within a site are highly positively or negatively correlated.

```{r}
# Normalization function -- squared sums of vector = 1
scalar1 <- function(x) {x / sqrt(sum(x^2))}

# Defining dot-product function
dotprod <- function(x, normalize = TRUE){
  
  # First, select the appropriate treatment and remove columns that aren't plant response
  N_vec <- x %>% filter(trt == "trt_N") %>% select(-trt, -site)
  P_vec <- x %>% filter(trt == "trt_P") %>% select(-trt, -site)
  K_vec <- x %>% filter(trt == "trt_K") %>% select(-trt, -site)
  
  # If normalizing, run the normalize function on each vector
  if(normalize == TRUE){
    N_vec <- scalar1(N_vec)
    P_vec <- scalar1(P_vec)
    K_vec <- scalar1(K_vec)
  }
  
  # Generate output dataframe of dot products (pracma::dot function)
  output = data.frame(NP = pracma::dot(as.numeric(N_vec), as.numeric(P_vec)),
                      NK = pracma::dot(as.numeric(N_vec), as.numeric(K_vec)),
                      PK = pracma::dot(as.numeric(P_vec), as.numeric(K_vec)))
  
  return(output)
  
}

# Run over our dataset
dotoutput = specscores_full %>% group_by(site) %>%
  do(dotprod(.))

# Load permANOVA p-value output
site_pvals = read.csv("composition_pvalues.csv", header = TRUE, stringsAsFactors = FALSE)

# Join all together (sites, treatments, dot-product pairs, perMANOVA p-values)
dot_full = left_join(site_pvals %>% select(-Total, -Residuals), dotoutput) %>% select(-X) %>%
  rename("trt_K" = "trt_K_num", "trt_P" =  "trt_P_num", "trt_N" = "trt_N_num")

# Write final data product
write.csv(x = dot_full, "../data/dot_full.csv")
```

In these histograms, I've only showed sites where both nutrient additions in each pair had significant effects in the permutational ANOVA.

I think this output highlights two key patterns:

- Species responses to N & P fertilization and N & K fertilization are not particularly correlated

- Species responses to P & K fertilization tend to be correlated with one another.

```{r, fig.height = 4, fig.width = 10}
p1 = dot_full %>% filter(trt_N < .05 & trt_P < .05) %>%
  ggplot(aes(x = NP)) +
  geom_histogram(fill = "white", color = "black") +
  xlim(-1, 1) +
  ggtitle("NP Correlation")

p2 = dot_full %>% filter(trt_N < .05 & trt_K < .05) %>%
  ggplot(aes(x = NK)) +
  geom_histogram(fill = "white", color = "black") +
  xlim(-1, 1) +
  ggtitle("NK Correlation")

p3 = dot_full %>% filter(trt_K < .05 & trt_P < .05) %>%
  ggplot(aes(x = PK)) +
  geom_histogram(fill = "white", color = "black") +
  xlim(-1, 1) +
  ggtitle("PK Correlation")

grid.arrange(p1,p2,p3,
             top = textGrob("Site Response Correlations",gp=gpar(fontsize=20,font=3)),
             nrow = 1)
```

To get a sense of what sites seem to be showing the strongest or weakest
tradeoffs, I've ranked these scores and plotted them with site labels. Each bar
is filled when the results of the permutational ANOVA indicate that both elements
of the fertilization tradeoff were significant. This figure may not make a whole
lot of sense without prior knowledge of the site distribution, but is useful in
thinking about what sites might show the biggest tradeoff.

```{r, fig.height = 15, fig.width = 15}
p1 = dot_full %>% arrange(NP) %>% 
  ggplot(aes(x = c(1:nrow(dot_full)),y = NP,
             fill = trt_P < 0.05 & trt_N < 0.05)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = site), angle = 90) +
  ggtitle("NP Correlation")+
  guides(fill = FALSE) +
  xlab("Site")+
  ylim(-1, 1.2)

p2 = dot_full %>% arrange(NK) %>%
  ggplot(aes(x = c(1:nrow(dot_full)),y = NK,
             fill = trt_K < 0.05 & trt_N < 0.05)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = site), angle = 90) +
  ggtitle("NK Correlation")+
  guides(fill = FALSE) +
  xlab("Site") +
  ylim(-1, 1.2)

p3 = dot_full %>% arrange(PK) %>%
  ggplot(aes(x = c(1:nrow(dot_full)),y = PK,
             fill = trt_P < 0.05 & trt_K < 0.05)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = site), angle = 90) +
  ggtitle("PK Correlation") +
  guides(fill = FALSE) +
  xlab("Site")+
  ylim(-1, 1.2)

grid.arrange(p1,p2,p3,
             top = textGrob("Site Response Correlations Filled By Significance",gp=gpar(fontsize=20,font=3)),
             nrow = 2)
```

I've also highlighted a couple of these below, plotting an RDA of the same
model used to generate the species responses to the different nutrients. This is
mostly a proof of concept. Are the values we extract from this metric pretty similar
to our results if we just examine the angles between vectors in an ordination?

```{r, fig.width= 12, fig.height= 5}
rdaFit = function(site){
  com.subset <- data.frame(cover.mat)[attr.mat$site_code == site,]
  
  # Remove all zero columns
  com.subset = com.subset[,colSums(com.subset) > 0]
  
  # Pull out relevant plot attributes for each site
  attr.subset <- data.frame(attr.mat) %>% filter(site_code == site)
  
  # Check that these two matrices are the same size
  expect_true(nrow(attr.subset) == nrow(com.subset))
  
  # Generate and return an NMDS figure
  mod_out = vegan::rda(com.subset ~ as.factor(year_trt) + trt_K_num + trt_P_num + trt_N_num,
                     data = attr.subset)
  
  return(mod_out)
  
}

par(mfrow = c(1, 3))

plot(rdaFit("cowi.ca"), main = "Cowichan - Strong Tradeoff, value = -0.5")
plot(rdaFit("sier.us"), main = "Sierra Footill - Weak Tradeoff, value = 0.1")
plot(rdaFit("konz.us"), main = "Konza - No Tradeoff, value = 0.9")
```

# What variables are correlated with different tradeoff axes?

```{r, echo = FALSE}
div.sum = soil.chars %>% 
  group_by(site_code) %>%
  filter(year_trt == 0) %>%
  summarise(pretrt_rich = unique(site_richness),
            meanprod = mean(na.omit(total_mass)),
            pretrt_simp = mean(inverse_simpson),
            mean_precip = unique(MAP_v2),
            mean_precipvar = unique(MAP_VAR_v2))

full_table = dot_full %>% rename(site_code = site) %>% 
  left_join(left_join(site.covars, div.sum, by = "site_code"))
```

## Simple univariate exploration

Rather than opting for a regression tree (or causal inference tree, as suggested
by the UC Davis statistical consultant) 

```{r, N - P}
p1 = full_table %>% filter(trt_N < 0.05 & trt_P < 0.05) %>% 
  ggplot(aes(x = pretrt_rich, y = NP)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE) +
  ylab("NP Correlation") +
  xlab("PreTrt Richness")

p2 = full_table %>% filter(trt_N < 0.05 & trt_P < 0.05) %>% 
  ggplot(aes(x = pretrt_simp, y = NP)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE)+
  ylab("NP Correlation") +
  xlab("PreTrt Simpson")

p3 = full_table %>% filter(trt_N < 0.05 & trt_P < 0.05) %>% 
  ggplot(aes(x = meanprod, y = NP)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE)+
  ylab("NP Correlation") +
  xlab("Mean Productivity")

p4 = full_table %>% 
  filter(trt_N < 0.05 & trt_P < 0.05) %>%
  ggplot(aes(x = cv.pctN, y = NP)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE)+
  ylab("NP Correlation") +
  xlab("%N Coef Var.")

p5 = full_table %>% 
  filter(trt_N < 0.05 & trt_P < 0.05) %>% 
  ggplot(aes(x = cv.ppmP, y = NP)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE)+
  ylab("NP Correlation") +
  xlab("ppm P Coef Var")

p6 = full_table %>% 
  filter(trt_N < 0.05 & trt_P < 0.05) %>% 
  ggplot(aes(x = mean_precip, y = NP)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE)+
  ylab("NP Correlation") +
  xlab("Mean Precip")

p7 = full_table %>% 
  filter(trt_N < 0.05 & trt_P < 0.05) %>% 
  ggplot(aes(x = mean_precipvar, y = NP)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE)+
  ylab("NP Correlation") +
  xlab("Precip Coef Var")

grid.arrange(p1,p2,p3,p4,p5,p6,p7,
             top = textGrob("N-P Response Correlation",gp=gpar(fontsize=20,font=3)))
```

```{r, N - K}
p1 = full_table %>% filter(trt_N < 0.05 & trt_K < 0.05) %>% 
  ggplot(aes(x = pretrt_rich, y = NP)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE) +
  ylab("NK Correlation") +
  xlab("PreTrt Richness")

p2 = full_table %>% filter(trt_N < 0.05 & trt_K < 0.05) %>% 
  ggplot(aes(x = pretrt_simp, y = NP)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE)+
  ylab("NK Correlation") +
  xlab("PreTrt Simpson")

p3 = full_table %>% filter(trt_N < 0.05 & trt_K < 0.05) %>% 
  ggplot(aes(x = meanprod, y = NP)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE)+
  ylab("NK Correlation") +
  xlab("Mean Productivity")

p4 = full_table %>% 
  filter(trt_N < 0.05 & trt_K < 0.05) %>%
  ggplot(aes(x = cv.pctN, y = NP)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE)+
  ylab("NK Correlation") +
  xlab("%N Coef Var.")

p5 = full_table %>% 
  filter(trt_N < 0.05 & trt_K < 0.05) %>% 
  ggplot(aes(x = cv.ppmK, y = NP)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE)+
  ylab("NK Correlation") +
  xlab("ppm K Coef Var")

p6 = full_table %>% 
  filter(trt_N < 0.05 & trt_K < 0.05) %>% 
  ggplot(aes(x = mean_precip, y = NP)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE)+
  ylab("NK Correlation") +
  xlab("Mean Precip")

p7 = full_table %>% 
  filter(trt_N < 0.05 & trt_K < 0.05) %>% 
  ggplot(aes(x = mean_precipvar, y = NP)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE)+
  ylab("NK Correlation") +
  xlab("Precip Coef Var")

grid.arrange(p1,p2,p3,p4,p5,p6,p7,
             top = textGrob("N-K Response Correlation",gp=gpar(fontsize=20,font=3)))
```

```{r, P - K}
p1 = full_table %>% filter(trt_K < 0.05 & trt_P < 0.05) %>% 
  ggplot(aes(x = pretrt_rich, y = PK)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE) +
  ylab("PK Correlation") +
  xlab("PreTrt Richness")

p2 = full_table %>% filter(trt_K < 0.05 & trt_P < 0.05) %>% 
  ggplot(aes(x = pretrt_simp, y = PK)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE)+
  ylab("PK Correlation") +
  xlab("PreTrt Simpson")

p3 = full_table %>% filter(trt_K < 0.05 & trt_P < 0.05) %>% 
  ggplot(aes(x = meanprod, y = PK)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE)+
  ylab("PK Correlation") +
  xlab("Mean Productivity")

p4 = full_table %>% 
  filter(trt_K < 0.05 & trt_P < 0.05) %>%
  ggplot(aes(x = cv.ppmK, y = PK)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE)+
  ylab("PK Correlation") +
  xlab("ppm K Coef Var.")

p5 = full_table %>% 
  filter(trt_K < 0.05 & trt_P < 0.05) %>% 
  ggplot(aes(x = cv.ppmP, y = PK)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE)+
  ylab("PK Correlation") +
  xlab("ppm P Coef Var")

p6 = full_table %>% 
  filter(trt_K < 0.05 & trt_P < 0.05) %>% 
  ggplot(aes(x = mean_precip, y = PK)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE)+
  ylab("PK Correlation") +
  xlab("Mean Precip")

p7 = full_table %>% 
  filter(trt_K < 0.05 & trt_P < 0.05) %>% 
  ggplot(aes(x = mean_precipvar, y = PK)) +
  geom_point() +
  stat_smooth(se = FALSE, method = "lm") +
  guides(color = FALSE)+
  ylab("PK Correlation") +
  xlab("Precip Coef Var")

grid.arrange(p1,p2,p3,p4,p5,p6,p7,
             top = textGrob("P - K Response Correlation", gp=gpar(fontsize=20,font=3)))
```




