---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

# Directional Change Updates

Hi Siddarth, thanks again for being patient as it's taken me a while to get updates back.

As a refresher, here is what I think are some key questions to ask with respect to directional change in NutNet data. In my mind, these are the key outcomes that are often presented as the consequences of chronic nutrient enrichment, wherein communities become less diverse and increasingly dominated by the subset of species capable of outcompeting others when nutrient limitation is reduced. 

1. Does nutrient enrichment produce a decrease in community diversity over time?

2. Does nutrient enrichment increase within-year dissimilarity over time?
(Irrespective of directional change, does nutrient enrichment make communities more different from one another?)

3. Does nutrient enrichment increase rate of temporal community turnover? 
(Implies, but does not directly describe directional change)

4. How do nutrient enrichment and environmental covariates differentially explain variation in community similarity over time?

Together, I think this paints a picture of how communities are responding to nutrient enrichment over time -- whether communities are producing the consistent response we expect from chronic nutrient enrichment, wherein nutrient enrichment effects increase over time, producing directional change in community composition that consistently favors a subset of species.

To answer these questions, I've done a bit of reading on methodology (file attached). To summarize my thoughts on how to answer these different questions:

## *1. Diversity change* 

I think this is the most straightforward - there are a number of potential diversity statistics to be used, though I'm most partial to Hill Numbers / ENS approaches. With an index of choice, I see a simple linear mixed-effects model to be the best approach, relating diversity over time to treatment (or number of years * treatment), with site (and if relevant, nested blocks) as random effects.

## *2. Within year compositional dissimilarity*

There are a number of different approaches to determining beta diversity within a given year. While one approach may be to calculate the value of beta in an ENS / Hill Numbers approach, I think this multiplicative definition of beta diversity doesn't do a great job answering this question. Instead, a dissimilarity-focused approach will be more appropriate.

A table presented in Magurran and McGlinn (2011) sums up different dissimilarity metrics and approaches to multiple pairwise comparisons very nicely. The authors of this chapter really seem to emphasize use of Horn, Morisita-Horn, or Percentage Dissimilarity indices over Bray-Curtis, which seems to have some poor mathematical properties. Regardless of which dissimilarity metric is chosen, there are a number of different methods to calculate dissimilarity among multiple assemblages.

* Bray-Curtis Dissimilarity: If using B-C to determine similarity among multiple assemblages, the choices appear to be either taking a mean of pairwise similarities or distance to the centroid, as calculated in the betadisper() function.

* Horn or Morisita-Horn Dissimilarity: Both of these indices have a defined method of determining multiple-assemblage dissimilarity, producing a single statistic for each group of communities.

In either case, I think a simple way to test whether nutrient enrichment increases within-year dissimilarity over time is to again construct a mixed-effects model relating dissimilarity to time, with random site (and potentially nested block) effects. 

## *3. Temporal turnover rate*

Temporal turnover of communities appears to be a difficult process to capture using a simple metric, particularly in instances where the number of datapoints isn't particularly large, fail to exhibit regular fluctuations, or don't present a simple unidirectional response.

As a result, there are a number of proposed metrics to assess community turnover rate, each seeming to emphasize different inference. Some focus on decomposing the source of community variation to changes in relative abundance and total individual density, while others seem best at total variation in a community over a the course of a sampling period.

I think that in our case, an interest in directional change suggests the use of a standard dissimilarity vs. temporal lag approach. In Magurran and McGlinn (2011), the chapter presented on temporal diversity provides some more detail- models that include nonlinear fits and non-normal variance structure may be necessary, as variance is often inflated at larger temporal lags. I haven't found anything that explicitly states euclidean distance (the metric of choice in the "codyn" package) vs. any other distance metric is needed, provided basic model assumptions aren't violated.

While I think this approach is better at suggesting directional change than a metric like species-exchange-ratio, I think that there are alternative patterns of community change that may still produce an increase in the slope of this relationship that may be taken as directional change. For example, if nutrient enrichment increases random community variation in a temporally autocorrelated fashion, the slope of the relationship between temporal lag and community distance can still increase relative to controls.

## *4. Explaining community variation through ordination*

To address how well our nutrient addition tratments capture observed community variation, I think the best approach is some combination of multivariate ANOVA, unconstrained ordination, and constrained ordination. 

From a visualization standpoint, NMDS should suffice as an effective unconstrained ordination method to capture potential trends.

For constrained ordination, in which we can try to parse out the maximum amount of community variation possible given a set of explanatory variables. 

A number of constrained ordination methods are supported in Vegan, though the amount of inference seems to vary depending on how novel the approach is (CAP, for example, seems to be less supported than CCA or RDA). 

Some thoughts on ways to proceed:

* Partitioning based approaches? Can try to determine what fraction of community variation can be explained by one matrix (treatments) given another (environmental covariaties, time, etc.). 

  * One approach is partial RDA, where the influence of covariates is first accounted for in constrained ordination (as well as the fraction of variation that is redundantly accounted for with both treatments and covariates), then the remaining variation is constrained by treatment variables.

  * Another related approach, called "variance partitioning" in Vegan, does a similar sort of analysis to the above, but presents the components of variance that are accounted for by treatments and environmental covariates, respectively. 

* Alternatively, one could try to test whether the addition of treatment as a parameter significantly improves model fit, or compare the relative fraction of variation explained by adding this parameter. There are a number of questions on how to better implement this approach, but we can hash out specifics later.

* A final, more complicated approach that stuck out to me were Moran's eigenvector maps (MEMs) that attempt to account for spatially or temporally autocorrelated community matrices. While interesting, I think these are too complex for our use.

In turn, all these analysis can be supplemented using a permutational ANOVA, which can also parse out the influence of time, block, and nutrient addition effects on a given set of community dissimilarities.

## Questions and thoughts on future direction

__A note on comparison:__ One clear complication is that the more closely we want to associate temporal turnover with specific environmental variables, the more difficult it becomes to make comparisons between sites. While this I think that a focus on explanatory climatic variables is interesting, I think there's significant difficulty in comparing sites where the best set of explantory variables differs greatly. Simply treating year as a factor variable, rather than decomposing it into environmental variables, may be a solution.   

On the other hand, a more climate-focused analysis could lend itself to some very interesting questions, particularly if we're interested in focusing just on the California sites. Rather than trying to make comparisons, it may also be fruitful to ask how the recent California drought has interacted with nutrient enrichment or masked its effects, and whether these responses to drought have differed across sites. I would guess that to answer that question, we may need more detailed climate information, perhaps to combine ETo and precipitation to determine drought stress, or presence of periodic droughts, etc.  

I think that either a focus on comparing sites more coarsely, or on a more detailed exploration of climate as a driver of community variation would be interesting, but use somewhat different approaches.  

Besides your thoughts on these suggested analysis steps, I have a few more questions about the data you sent along: 

- What are the units of precipitation and ETo in this dataset?

- What are the relevant distinctions in water years (if different) for sites in the Great Plains?

- How do you best suggest using ETo? Is it more meaningful to focus on ETo values for an entire year, or estimate growing season evapotranspiration potential?


# Reading in community data

```{r, message = FALSE, echo = FALSE, warning = FALSE}
################################################################################
### Data preprocessing

# Loading necessary packages
library(codyn) # Temporal analysis
library(ggplot2) # Figure plotting
library(tidyverse) # Data manipulation
library(lubridate)
library(data.table)
library(vegetarian) # Exponentiated diversity indices (Hill Numbers)
library(lme4) # Linear mixed effects models
library(vegan)

################################################################################
# Loading in dataset (up to 2017 data)
cov.long <- fread('C:/Users/ebatz/Dropbox/NutNet Data/full-cover-09-April-2018.csv',
                  na.strings = c('NA','NULL'))

# Subsetting to just California sites
site.subset <- c("mcla.us", "hopl.us", "sier.us", "elliot.us", "sedg.us",
                 "cbgb.us", "cdcr.us", "konz.us", "saline.us", "sevi.us", "sgs.us", "temple.us", "trel.us")
cov.long <- cov.long[cov.long$site_code %in% site.subset,]

# Choose identifying variables
ids <- c('site_code','year','year_trt', 'block','plot','trt')

# Remove non-live percent cover esimates
cov.long[,max_cover:=as.numeric(max_cover)] 
cov.long <- cov.long[live==1]

# Cast long into wide
plot.sp.mat <- dcast(cov.long,site_code+year+block+plot+trt+year_trt ~ Taxon,value.var='max_cover', 
                     fun.aggregate = sum,drop=T,fill=0)

head(plot.sp.mat)[,1:10]
```

# Reading in Climate Data

Variables:

site_code: Site name
month: Month
year: Year
ppt: precipitation (units? mm?)
ETo: evapotranspiration (units? mm?)

```{r}
climate <- read.csv("C:/Users/ebatz/Box Sync/Eviner Lab shared/Evan/Research Projects/NutNet/Data/sites-precip_ETo_20180228.csv")
head(climate)
```

__Sites Represented__:

```{r}
clim_sites <- unique(climate$site_code)
clim_sites
```

## Aggregating by Water Year:

__ What are the general patterns in precipitation and ETo among the sites?__

```{r}
def_wtryear = function(x){
  if(x[7] == "CA"){
    wtryr <- if_else(as.numeric(x[3]) >= 9, as.numeric(x[4]) + 1, as.numeric(x[4]))
  }else if(x[7] == "MW"){
    wtryr <- if_else(as.numeric(x[3]) >= 9, as.numeric(x[4]) + 1, as.numeric(x[4]))
  }else{
    stop("Site must be 'CA' or 'MW'")
  }
  return(wtryr)
}

ca.sites = c("mcla.us", "hopl.us", "sier.us", "elliot.us", "sedg.us")

climate$loc = if_else(climate$site_code %in% ca.sites, "CA", "MW")

climate$wtryr = apply(climate, 1, def_wtryear)

climate_agg <- climate %>% group_by(site_code, wtryr, loc) %>%  
  summarise(tot_precip = sum(ppt), tot_ETo = sum(ETo)) 

ungroup(climate_agg) %>%  
filter(wtryr >= (as.numeric(min(plot.sp.mat$year))) - 1) %>%
ggplot(., aes(x = wtryr, y = tot_precip, color = site_code)) + 
  geom_line() + 
  facet_wrap(~loc) +
  ggtitle("Mean Annual Precipitation")
```

As expected, it appears the 2012-2015 California drought is pretty apparent. It's also nice to see that there seems to be a roughly equivalent distribution of mean annual precipitation values across the sets of sites:

```{r}
climate_agg %>%
  group_by(site_code, loc) %>%
  summarise(meanprecip = mean(na.omit(tot_precip)),
            meanETo = mean(na.omit(tot_ETo))) %>%
  arrange(loc, meanprecip) %>%
  ggplot(., aes(x = site_code, y = meanprecip, fill = loc))+
  geom_bar(stat = "identity")

climate_agg %>%
  group_by(site_code, loc) %>%
  summarise(meanprecip = mean(na.omit(tot_precip)),
            meanETo = mean(na.omit(tot_ETo))) %>%
  arrange(loc, meanprecip) %>%
  full_join(plot.sp.mat %>%
    group_by(site_code) %>%
    summarise(obs = n()))
  
```

# Creating some new climate variables that better reflect seasonality in precipitation and temperature, which is known to be a strong driver of community assembly in California.

While coarse, this is a first attempt at creating relevant environmental variables:

* Total precipitation for a given year

* Lagged total precipitation for a given year

* Winter precipitation

* Spring precipitation 

And associated values of ETo.

```{r}
climate_winter <- climate %>% group_by(site_code, wtryr, loc) %>%  
  filter(month >= 11 | month == 1) %>%
  summarise(win_precip = sum(ppt), win_ETo = mean(ETo)) 

climate_spring <- climate %>% group_by(site_code, wtryr, loc) %>%  
  filter(month <= 4 & month > 1) %>%
  summarise(spr_precip = sum(ppt), spr_ETo = mean(ETo)) 

climate_summer <- climate %>% group_by(site_code, wtryr, loc) %>%  
  filter(month <= 7 & month > 4) %>%
  summarise(sum_precip = sum(ppt), sum_ETo = mean(ETo)) 

climate_fall <- climate %>% group_by(site_code, wtryr, loc) %>%  
  filter(month <= 10 & month > 7) %>%
  summarise(fal_precip = sum(ppt), fal_ETo = mean(ETo)) 

climate_lagged <- climate %>%   
  mutate(wtryr = wtryr + 1) %>%
  group_by(site_code, wtryr, loc) %>%  
  summarise(lag_precip = sum(ppt), lag_ETo = mean(ETo)) 

climate_all <- Reduce(function(x, y) merge(x, y, all=TRUE), list(climate_agg, climate_lagged, 
                                                  climate_winter, climate_spring,
                                                  climate_summer, climate_fall))
names(climate_all)[2] = "year"

```


# Ordination

In this write-up, I'm just going to focus on trying to explain community variation in the California sites. Before tackling any of the rest, I think it'd be good to decide on our choice of approach, as well as meaningful ways to aggregate climate data for the midwest sites.

After generating a function that will produce a simple NMDS plot, I attempted to see how well very simple climate variables (mean annual precipitation, lagged mean annual precipitation, etc.) predicted within-site variation. This model isn't terrible - especially when adding a "block" term, but does not explain a huge amount of variance.

However, the more complicated the ordination approach, the more difficult it is to make site-by-site comparisons. While only incleading simple variables like total annual precipitation and average temperature, or treating time as a factor variable, may be less mechanistic, it's easier to fit the same model to multiple sites.

__Note:__ Rather than just subsetting our data to only contain information on control sites, it seemed better to try to detect this change across all datapoints. If we're interested in seeing how much total variation is explained using climate variables, I think that subsetting data (particularly when we have only a few obsevations), may cause issues.

# Unconstrained ordination (NMDS)

A quick attempt at determining general patterns in the data, rather than our ability to explain variation with climate and treatments. Ellipses are drawn by year, with points colored by treatment, and made more/less transparent based on the number of years of treatment. To remove the transparency, simply set "alpha = NULL" in line 341.

```{r}
# Calculating the shape of the ellipses
veganCovEllipse<-function (cov, center = c(0, 0), scale = 1, npoints = 100){
  theta <- (0:npoints) * 2 * pi/npoints
  Circle <- cbind(cos(theta), sin(theta))
  t(center + scale * t(Circle %*% chol(cov)))
}

pretty_NMDS <- function(sp.mat, env.mat, group){
  
  nmds_run = metaMDS(sp.mat,
                     try = 50,
                     trymax = 100,
                     trace = FALSE,
                     distance = "horn"
  )
  
  groupvar <- subset(env.mat, select = group)
  names(groupvar) = "group"
  NMDS = bind_cols(data.frame(nmds_run$points), groupvar)
  NMDS$group = as.factor(NMDS$group)
  NMDS.mean=aggregate(NMDS[,1:2], list(group=NMDS$group), mean)
  
  df_ell <- data.frame()
  for(g in levels(NMDS$group)){
    df_ell <- rbind(df_ell, cbind(as.data.frame(with(NMDS[NMDS$group==g,],
                                                     veganCovEllipse(cov.wt(cbind(MDS1,MDS2),
                                                                            wt=rep(1/length(MDS1),length(MDS1)))$cov,
                                                                     center=c(mean(MDS1),mean(MDS2)))))
                                  ,group=g))
  }
  
  # Adding other details for facetting
  NMDS = bind_cols(NMDS, env.mat)
  
  NMDS.mean = left_join(NMDS.mean, bind_cols(group = groupvar, env.mat[,1:7]),
                        by = "group")
  
  NMDS.mean = NMDS.mean[!duplicated(NMDS.mean[,2:2]),]
  
  return(list(NMDS, NMDS.mean, df_ell, nmds_run))
}

```

```{r}
siteName <- "mcla.us"
distmethod <- "bray"
trts <- c("Control", "N", "P", "K")

ord.sp.mat <- plot.sp.mat[plot.sp.mat$trt %in% trts & plot.sp.mat$site_code == siteName,]

# Combine climate data with community dataset
ord.sp.mat$year <- as.numeric(ord.sp.mat$year)
ord.sp.mat <- right_join(climate_all[!is.na(climate_agg$tot_precip),], ord.sp.mat, by = c("year", "site_code"))

sp.mat <- ord.sp.mat[ord.sp.mat$site_code == siteName, -(1:19)]
sp.mat <- wisconsin(sp.mat)
env.mat <- ord.sp.mat[ord.sp.mat$site_code == siteName, 1:19]

NMDS_output <- pretty_NMDS(sp.mat, env.mat, "year_trt")

NMDS = NMDS_output[[1]]
NMDS.mean = NMDS_output[[2]]
df_ell = NMDS_output[[3]]
nmds_run = NMDS_output[[4]]

# Producing figure
NMDS$year_trt = as.numeric(NMDS$year_trt)
NMDS$alpha = NMDS$year_trt / max(NMDS$year_trt)
ggplot(data = NMDS, aes(MDS1, MDS2)) + 
  geom_point(aes(color = NMDS$trt, alpha = alpha), size = 3) + 
  geom_path(data = df_ell, aes(x=MDS1, y=MDS2, group = group), size=1, linetype=1) +
  geom_text(data = NMDS.mean, aes(label = NMDS.mean$group)) + 
  theme_bw()+ 
  geom_text(data = data.frame(MDS1 = .95, MDS2 = -1.1), 
            aes(label = paste("Stress:", round(nmds_run$stress, 2))),
            fontface = "bold")
```

# Constrained ordination

To see how much variation in the community data we can explain using a set of climatic variables, I opted for a constrained ordination approach. We can have more discussions on appropriate distance metrics to use + constrained ordination methods, but I opted for Bray-Curtis in this case and CAPscale(), which allows for non-euclidean distances between communities. 

In this first run, I just tried to see how well community dissimilarity is predicted by 

* Total annual precipitation
* Total annual ETo
* Lagged annual precipitation
* Block and treatment effects

For most sites, this seems to be a fair model, but not great. Feel free to change "siteName" to whatever site code you would like to fit the model with the same set of variables, or change the code on line 395 for edits to the model.

```{r}
siteName <- "hopl.us"
distmethod <- "bray"
trts <- c("Control", "N", "P", "K")
plot.sp.mat$year_trt <- as.numeric(plot.sp.mat$year_trt)

ord.sp.mat <- plot.sp.mat[plot.sp.mat$trt %in% trts & plot.sp.mat$site_code == siteName,]

  # Combine climate data with community dataset
  ord.sp.mat$year <- as.numeric(ord.sp.mat$year)
  ord.sp.mat <- right_join(climate_all[!is.na(climate_agg$tot_precip),], ord.sp.mat, by = c("year", "site_code"))
  
  # Remove any NA columns
  ord.sp.mat <- na.omit(ord.sp.mat)
  
  # Select only those columns that contain species data
  sp.mat <- ord.sp.mat[ord.sp.mat$site_code == siteName, -(1:19)]
  
  # Standardize the rows of the dataset
  sp.mat <- wisconsin(sp.mat)
  
  # Select only those columns that contain environmental and site identification data
  env.mat <- ord.sp.mat[ord.sp.mat$site_code == siteName, 1:19]
  
  # Make sure that year is a numeric value, as opposed to factor
  env.mat$year_trt = as.numeric(env.mat$year_trt)
  
  # Adding dummy nutrient enrichment variables
  env.mat = bind_cols(env.mat, data.frame(dummy(env.mat$trt) * as.numeric(env.mat$year_trt)))
  
  # Running stepwise model selection for CAPscale
  mat.dist <- vegdist(sp.mat, method = "bray")
  mod0 <- capscale(mat.dist ~ (tot_precip + tot_ETo + lag_precip) + block + N + P + K, data = env.mat)
  mod0
  
  plot(mod0)
```

However, we can also let the model select relevant variables using forward stepwise selection. In the models below, I've fed the step() function all possible relevant parameters, and let the function choose the best subset using AIC. While these models tend to fit well, they are also quite complicated. I'm also a little hesitant to include too many vaiables given the number of datapoints we have.

Output for all California sites is given below.

```{r}
siteName <- "mcla.us"
distmethod <- "bray"
trts <- c("Control", "N", "P", "K")
plot.sp.mat$year_trt <- as.numeric(plot.sp.mat$year_trt)

cap_model  <- function(siteName, distmethod, trts, plot.sp.mat, climate_all){
  
  ord.sp.mat <- plot.sp.mat[plot.sp.mat$trt %in% trts & plot.sp.mat$site_code == siteName,]

  # Combine climate data with community dataset
  ord.sp.mat$year <- as.numeric(ord.sp.mat$year)
  ord.sp.mat <- right_join(climate_all[!is.na(climate_agg$tot_precip),], ord.sp.mat, by = c("year", "site_code"))
  
  # Remove any NA columns
  ord.sp.mat <- na.omit(ord.sp.mat)
  
  # Select only those columns that contain species data
  sp.mat <- ord.sp.mat[ord.sp.mat$site_code == siteName, -(1:19)]
  
  # Standardize the rows of the dataset
  sp.mat <- wisconsin(sp.mat)
  
  # Select only those columns that contain environmental and site identification data
  env.mat <- ord.sp.mat[ord.sp.mat$site_code == siteName, 1:19]
  
  # Make sure that year is a numeric value, as opposed to factor
  env.mat$year_trt = as.numeric(env.mat$year_trt)
  
  # Adding dummy nutrient enrichment variables
  env.mat = bind_cols(env.mat, data.frame(dummy(env.mat$trt) * as.numeric(env.mat$year_trt)))
  
  # Pulling out relevant variables (climate variables and trt, and year_trt)
  env.subset <- env.mat[,c(4:15, 20:22)]
  
  # Running stepwise model selection for CAPscale
  mat.dist <- vegdist(sp.mat, method = "bray")
  mod0 <- capscale(mat.dist ~ 1, data = env.subset)
  mod1 <- capscale(mat.dist ~ . * . , data = env.subset)
  mod_step <- step(mod0, scope = formula(mod1), test = "perm", trace = FALSE)
  
  # Return the fitted model
  return(mod_step)
  
}

mod.output <- cap_model(siteName, distmethod, trts, plot.sp.mat, climate_all)

mod.output

plot(mod.output)
```

```{r}
siteName <- "hopl.us"
distmethod <- "bray"
trts <- c("Control", "N", "P", "K")

mod.output <- cap_model(siteName, distmethod, trts, plot.sp.mat, climate_all)
mod.output
plot(mod.output)
```

```{r}
siteName <- "elliot.us"
distmethod <- "bray"
trts <- c("Control", "N", "P", "K")

mod.output <- cap_model(siteName, distmethod, trts, plot.sp.mat, climate_all)
mod.output
plot(mod.output)
```

```{r}
siteName <- "sedg.us"
distmethod <- "bray"
trts <- c("Control", "N", "P", "K")

mod.output <- cap_model(siteName, distmethod, trts, plot.sp.mat, climate_all)
mod.output
plot(mod.output)
```

```{r}
siteName <- "sier.us"
distmethod <- "bray"
trts <- c("Control", "N", "P", "K")

mod.output <- cap_model(siteName, distmethod, trts, plot.sp.mat, climate_all)
mod.output
plot(mod.output)
```
